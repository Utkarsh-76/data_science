{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class crawler:\n",
    "    # Initialize the crawler with the name of database \n",
    "    def __init__(self,dbname):\n",
    "        self.con=sqlite.connect(dbname)\n",
    "    \n",
    "    def __del__(self): \n",
    "        self.con.close()\n",
    "    def dbcommit(self):\n",
    "        self.con.commit()\n",
    "    \n",
    "    # Auxilliary function for getting an entry id and adding\n",
    "    # it if it's not present\n",
    "    def getentryid(self,table,field,value,createnew=True):\n",
    "        cur=self.con.execute(\"select rowid from %s where %s='%s'\" % (table,field,value)) \n",
    "        res=cur.fetchone( )\n",
    "        if res==None:\n",
    "            cur=self.con.execute(\"insert into %s (%s) values ('%s')\" % (table,field,value))\n",
    "            return cur.lastrowid\n",
    "        else:\n",
    "            return res[0]\n",
    "    \n",
    "    # Index an individual page\n",
    "    def addtoindex(self,url,soup):\n",
    "        print('Indexing %s' % url)\n",
    "        \n",
    "    # Extract the text from an HTML page (no tags)\n",
    "    def gettextonly(self,soup):\n",
    "        v=soup.string\n",
    "        if v==None:\n",
    "            c=soup.contents\n",
    "            resulttext=''\n",
    "            for t in c:\n",
    "                subtext=self.gettextonly(t)\n",
    "                resulttext+=subtext+'\\n'\n",
    "            return resulttext\n",
    "        else:\n",
    "            return v.strip()\n",
    "    \n",
    "    # Separate the words by any non-whitespace character\n",
    "    def separatewords(self,text):\n",
    "        splitter=re.compile('\\\\W*')\n",
    "        return [s.lower() for s in splitter.split(text) if s!='']\n",
    "\n",
    "    # Return true if this url is already indexed\n",
    "    def isindexed(self,url):\n",
    "        u=self.con.execute(\"select rowid from urllist where url='%s'\" % url).fetchone() \n",
    "        \n",
    "        if u!=None:\n",
    "        # Check if it has actually been crawled\n",
    "            v=self.con.execute('select * from wordlocation where urlid=%d' % u[0]).fetchone() \n",
    "            if v!=None: return True\n",
    "        return False\n",
    "    \n",
    "    # Add a link between two pages\n",
    "    def addlinkref(self,urlFrom,urlTo,linkText):\n",
    "        if self.isindexed(url): return\n",
    "        print('Indexing '+url)\n",
    "\n",
    "        # Get the individual words\n",
    "        text=self.gettextonly(soup)\n",
    "        words=self.separatewords(text)\n",
    "\n",
    "        # Get the URL id\n",
    "        urlid=self.getentryid('urllist','url',url)\n",
    "\n",
    "        # Link each word to this url\n",
    "        for i in range(len(words)):\n",
    "            word=words[i]\n",
    "            if word in ignorewords: continue\n",
    "            wordid=self.getentryid('wordlist','word',word)\n",
    "            self.con.execute(\"insert into wordlocation(urlid,wordid,location) \\ values (%d,%d,%d)\" % (urlid,wordid,i))\n",
    "    \n",
    "    # Starting with a list of pages, do a breadth\n",
    "    # first search to the given depth, indexing pages\n",
    "    # as we go\n",
    "    def crawl(self,pages,depth=2):\n",
    "        newpages=set()\n",
    "        for page in pages:\n",
    "            try:\n",
    "                c=urllib2.urlopen(page)\n",
    "            except:\n",
    "                print(\"Could not open %s\" % page)\n",
    "                continue\n",
    "            soup=BeautifulSoup(c.read(),'lxml') \n",
    "            self.addtoindex(page,soup)\n",
    "            \n",
    "            links=soup('a')\n",
    "            for link in links:\n",
    "                if ('href' in dict(link.attrs)):\n",
    "                    url=urljoin(page,link['href'])\n",
    "                    if url.find(\"'\")!=-1: continue\n",
    "                    url=url.split('#')[0]  # remove location portion\n",
    "                    if url[0:4]=='http' and not self.isindexed(url):\n",
    "                        newpages.add(url)\n",
    "                    linkText=self.gettextonly(link)\n",
    "                    self.addlinkref(page,url,linkText)\n",
    "                self.dbcommit( ) \n",
    "            pages=newpages\n",
    "    \n",
    "    # Create the database tables\n",
    "    def createindextables(self):\n",
    "        self.con.execute('create table urllist(url)')\n",
    "        self.con.execute('create table wordlist(word)')\n",
    "        self.con.execute('create table wordlocation(urlid,wordid,location)')\n",
    "        self.con.execute('create table link(fromid integer,toid integer)')\n",
    "        self.con.execute('create table linkwords(wordid,linkid)')\n",
    "        self.con.execute('create index wordidx on wordlist(word)')\n",
    "        self.con.execute('create index urlidx on urllist(url)')\n",
    "        self.con.execute('create index wordurlidx on wordlocation(wordid)')\n",
    "        self.con.execute('create index urltoidx on link(toid)') \n",
    "        self.con.execute('create index urlfromidx on link(fromid)') \n",
    "        self.dbcommit( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import *\n",
    "from urlparse import urljoin\n",
    "\n",
    "# Create a list of words to ignore\n",
    "ignorewords=set(['the','of','to','and','a','in','is','it'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysqlite2 import dbapi2 as sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler=crawler(\"searchindex.db\")\n",
    "crawler.createindextables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unbound method crawl() must be called with crawler instance as first argument (got list instance instead)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0bb6356334f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'https://guides.github.com/activities/hello-world/'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcrawler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unbound method crawl() must be called with crawler instance as first argument (got list instance instead)"
     ]
    }
   ],
   "source": [
    "pages=['https://guides.github.com/activities/hello-world/']\n",
    "crawler.crawl(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
